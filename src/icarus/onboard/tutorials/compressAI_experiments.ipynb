{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0 prepare data and libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sunpy zeep drms hvpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pytorch_msssim compressai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hvpy\n",
    "import matplotlib.pyplot as plt\n",
    "from sunpy.time import parse_time\n",
    "from sunpy.util.config import get_and_create_download_dir\n",
    "from matplotlib.image import imread\n",
    "import math, io, os, torch\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from pytorch_msssim import ms_ssim\n",
    "from compressai.zoo import bmshj2018_factorized\n",
    "from ipywidgets import interact, widgets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download sample image from STEREO\n",
    "cor2_file = hvpy.save_file(hvpy.getJP2Image(parse_time('2014/05/15 07:54').datetime,\n",
    "                                            hvpy.DataSource.COR2_A.value),\n",
    "                           get_and_create_download_dir() + \"/COR2.jp2\")\n",
    "print(cor2_file)\n",
    "!cp /root/sunpy/data/COR2.jp2 COR2.jp2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -luah\n",
    "# We downloaded example image \"COR2.jp2\" with 265K (it's a JPEG 2000 file - the real raw data might be different)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = imread(\"COR2.jp2\")\n",
    "print(img.shape, img.dtype)\n",
    "plt.imshow(img, cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 on-board (compress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/InterDigitalInc/CompressAI/blob/master/examples/CompressAI%20Inference%20Demo.ipynb\n",
    "# model -> https://github.com/InterDigitalInc/CompressAI/blob/b10cc7c1c51a0af26ea5deae474acfd5afdc1454/compressai/models/google.py\n",
    "\n",
    "# device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device = 'cpu'\n",
    "\n",
    "net = bmshj2018_factorized(quality=2, pretrained=True).eval().to(device)\n",
    "print(f'Parameters: {sum(p.numel() for p in net.parameters())}')\n",
    "\n",
    "img = imread(\"COR2.jp2\")\n",
    "print(\"original data range (min,mean,max):\", np.min(img), np.mean(img), np.max(img)) # 0-255\n",
    "\n",
    "img = np.asarray([img,img,img]) # fake rgb\n",
    "img = np.transpose(img, (1, 2, 0))\n",
    "\n",
    "x = transforms.ToTensor()(img)\n",
    "x = x.unsqueeze(0).to(device)\n",
    "print(\"x data range (min,mean,max):\", torch.min(x), torch.mean(x), torch.max(x)) # 0-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Full pass: out_net = net.forward(x)\n",
    "    # Compress:\n",
    "    print(\"x\", x.shape)\n",
    "    y = net.g_a(x)\n",
    "    print(\"y\", y.shape)\n",
    "    y_strings = net.entropy_bottleneck.compress(y)\n",
    "    print(\"len(y_strings) = \",len(y_strings[0]))\n",
    "\n",
    "    strings = [y_strings]\n",
    "    shape = y.size()[-2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(strings[0][0]))\n",
    "# print(shape)\n",
    "# name = \"latent_\" + str(shape[0])+\"_\"+str(shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save compressed forms:\n",
    "with open(name+\".bytes\", 'wb') as f:\n",
    "    f.write(strings[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -luah"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 ground-based (decompress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(name+\".bytes\", \"rb\") as f:\n",
    "    strings_loaded = f.read()\n",
    "strings_loaded = [[strings_loaded]]\n",
    "\n",
    "a, b = int(name.split(\"_\")[1]), int(name.split(\"_\")[2])\n",
    "shape_loaded = ([a,b])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "with torch.no_grad():\n",
    "    out_net = net.decompress(strings_loaded, shape_loaded)\n",
    "    #(is already called inside) out_net['x_hat'].clamp_(0, 1)\n",
    "\n",
    "x_hat = out_net['x_hat']\n",
    "print(\"x_hat data range (min,mean,max):\", torch.min(x_hat), torch.mean(x_hat), torch.max(x_hat)) # 0-1\n",
    "\n",
    "print(out_net.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_net = transforms.ToPILImage()(out_net['x_hat'].squeeze().cpu())\n",
    "print(\"reconstruction data range (min,mean,max):\", np.min(rec_net), np.mean(rec_net), np.max(rec_net)) # 0-255 again\n",
    "\n",
    "diff = torch.mean((out_net['x_hat'] - x).abs(), axis=1).squeeze().cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fix, axes = plt.subplots(1, 3, figsize=(16, 12))\n",
    "for ax in axes:\n",
    "    ax.axis('off')\n",
    "\n",
    "axes[0].imshow(img)\n",
    "axes[0].title.set_text('Original')\n",
    "\n",
    "axes[1].imshow(rec_net)\n",
    "axes[1].title.set_text('Reconstructed')\n",
    "\n",
    "axes[2].imshow(diff, cmap='viridis')\n",
    "axes[2].title.set_text('Difference')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_psnr(a, b):\n",
    "    mse = torch.mean((a - b)**2).item()\n",
    "    return -10 * math.log10(mse)\n",
    "\n",
    "def compute_msssim(a, b):\n",
    "    return ms_ssim(a, b, data_range=1.).item()\n",
    "\n",
    "def compute_bpp(out_net):\n",
    "    size = out_net['x_hat'].size()\n",
    "    num_pixels = size[0] * size[2] * size[3]\n",
    "    return sum(torch.log(likelihoods).sum() / (-math.log(2) * num_pixels)\n",
    "              for likelihoods in out_net['likelihoods'].values()).item()\n",
    "\n",
    "def convert_size(size_bytes):\n",
    "   if size_bytes == 0:\n",
    "       return \"0B\"\n",
    "   size_name = (\"B\", \"KB\", \"MB\", \"GB\", \"TB\", \"PB\", \"EB\", \"ZB\", \"YB\")\n",
    "   i = int(math.floor(math.log(size_bytes, 1024)))\n",
    "   p = math.pow(1024, i)\n",
    "   s = round(size_bytes / p, 2)\n",
    "   return \"%s %s\" % (s, size_name[i])\n",
    "\n",
    "def files_size(file_path):\n",
    "    size_bytes = os.path.getsize(file_path)\n",
    "    print(\"File\", file_path, \"has\", convert_size(size_bytes))\n",
    "    return size_bytes\n",
    "\n",
    "print(f'PSNR: {compute_psnr(x, out_net[\"x_hat\"]):.2f}dB')\n",
    "print(f'MS-SSIM: {compute_msssim(x, out_net[\"x_hat\"]):.4f}')\n",
    "if 'likelihoods' in out_net.keys():\n",
    "    print(f'Bit-rate: {compute_bpp(out_net):.3f} bpp')\n",
    "\n",
    "original_size = files_size(\"COR2.jp2\")\n",
    "latent_size = files_size(\"latent_128_128.bytes\")\n",
    "\n",
    "reduction_factor = original_size / latent_size\n",
    "print(\"Compressed with reduction factor by\", round(reduction_factor,2), \"times\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Notes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The data range of this loaded sample is well behaved (original data was between 0-255), our real world data will likely not be - normalisation between 0-1 before being passed to the network is needed\n",
    "\n",
    "- Real world data might have worse compression than this \".jp2\" sample\n",
    "\n",
    "- The model is pre-trained with RGB images - we waste these channels by repeating our one channel three times\n",
    "\n",
    "- This network is realtively small (11MB), but its speed needs to be tested on tiny devices. Evaluating it on something like the Myriad chip would also need rewriting a bit of the code to work in (almost) pure torch without many dependencies. For now, using just CPU on Colab VM, it takes about 12 sec to encode and 15 sec to decode - which is not very fast...\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
