{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377e97a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    File: Download_satellite_data.py\n",
    "    Authors: Martin Sanner, Josh Veitch-Michaelis\n",
    "    Created: 14.7.2023 - 15:14 CEST\n",
    "\n",
    "    Data Downloader creating the required folder structure locally to save data to for COR1, COR2 data for specified times.\n",
    "    Times are specified in the .ephemeris files, created using the Nasa HORIZONS tool. (.ephemeris are renamed text files)\n",
    "\n",
    "    Steps:\n",
    "        1: Load Ephemeris data\n",
    "        2: Estimate when 60Â° angles are hit\n",
    "        3: Gather data from COR1, COR2 on SECCHI\n",
    "        4: Create folder structure for COR1, COR2/media/josh/22893c92-cb6e-4f81-a50d-a9ff93ad5a811/data/fdl/2023/onboard/data/events/20140221_231212/cor1/20140218_231512_s4c1a.fts\n",
    "        5: Download data to that folder\n",
    "\"\"\"\n",
    "\n",
    "import logging\n",
    "import os\n",
    "from datetime import date, datetime, timedelta\n",
    "from functools import reduce\n",
    "from glob import glob\n",
    "from importlib.resources import files\n",
    "import time\n",
    "import shutil\n",
    "\n",
    "import astropy.io.fits as fits\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sscws\n",
    "import sunpy\n",
    "import yaml\n",
    "import logging\n",
    "from rich.progress import Progress\n",
    "from sunpy.net import Fido\n",
    "from sunpy.net import attrs as a\n",
    "from typing import List\n",
    "import astropy.table\n",
    "from astropy.time.core import Time\n",
    "from datetime import datetime, timedelta\n",
    "from rich.progress import track\n",
    "\n",
    "import astropy.io.fits as fitsio\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from natsort import natsorted\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3b69f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0075513f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ephemeris_data(fname: str) -> list:\n",
    "    \"\"\"\n",
    "    Loads data from horizon ephemeris files, passed as argument.\n",
    "    \"\"\"\n",
    "    with open(fname) as f:\n",
    "        data = f.readlines()\n",
    "\n",
    "    start_position = 0\n",
    "    end_position = 0\n",
    "    for i, line in enumerate(data):\n",
    "        if \"$$SOE\" in line:\n",
    "            start_position = i + 1\n",
    "        if \"$$EOE\" in line:\n",
    "            end_position = i\n",
    "\n",
    "    return data[start_position:end_position]\n",
    "\n",
    "\n",
    "def data_to_vectors(data, time_list: list = None):\n",
    "    \"\"\"\n",
    "    Takes the data returned by the load_data function and turns it into the required vectors\n",
    "    \"\"\"\n",
    "    time = time_list if time_list is not None else []\n",
    "    x = []\n",
    "    y = []\n",
    "    z = []\n",
    "    r = []\n",
    "\n",
    "    for line in data:\n",
    "        t, p = line.strip(\"\\n\").split(\"     \")\n",
    "        t2 = t.split(\" \")[1] + \" \" + t.split(\" \")[2]\n",
    "        if time_list is not None:\n",
    "            assert (\n",
    "                datetime.strptime(t2, \"%Y-%b-%d %H:%M\") in time_list\n",
    "            ), \"New Time found in line {}\".format(line)\n",
    "        else:\n",
    "            time.append(datetime.strptime(t2, \"%Y-%b-%d %H:%M\"))\n",
    "        space_split = [i for i in p.split(\" \") if i != \"\"]\n",
    "        RA = float(space_split[0]) * np.pi / 180  # Radian for numpy\n",
    "        DEC = float(space_split[1]) * np.pi / 180  # Radian for numpy\n",
    "\n",
    "        cr = float(space_split[2])  # in km\n",
    "        # dont use rdot here\n",
    "        cx = cr * np.cos(DEC) * np.cos(RA * np.cos(DEC))  # km\n",
    "        cy = cr * np.cos(DEC) * np.sin(RA * np.cos(DEC))  # km\n",
    "        cz = cr * np.sin(DEC)  # km\n",
    "\n",
    "        x.append(cx)\n",
    "        y.append(cy)\n",
    "        z.append(cz)\n",
    "        r.append(cr)\n",
    "    return time, x, y, z, r\n",
    "\n",
    "def download_batch(batch : List, folder : str) -> None:\n",
    "\n",
    "    filenames = []\n",
    "    try:\n",
    "        os.makedirs(folder, exist_ok=True, mode=0o777)\n",
    "    except OSError as e:\n",
    "        logging.error(\"Error when creating Folder = {} - {}\".format(folder, e))\n",
    "\n",
    "    try:\n",
    "        filenames = Fido.fetch(\n",
    "            batch,\n",
    "            path=\"{}/\".format(folder),\n",
    "            progress=False,\n",
    "            overwrite=True,\n",
    "            max_conn=8\n",
    "        )\n",
    "\n",
    "        if len(filenames) > 0:\n",
    "            Fido.fetch(\n",
    "                filenames,\n",
    "                path=\"{}/\".format(folder),\n",
    "                progress=False,\n",
    "                overwrite=True,\n",
    "                max_conn=8\n",
    "            )\n",
    "    except KeyboardInterrupt:\n",
    "        return\n",
    "    except Exception as e:\n",
    "        logging.error(\n",
    "            \"Error encountered in downloading batch: {}\".format(e)\n",
    "        )\n",
    "\n",
    "    return\n",
    "\n",
    "def get_events(min_time, max_time):\n",
    "    \"\"\"\n",
    "    Get events from HEK\n",
    "    \"\"\"\n",
    "    time_requested_for_batch = a.Time(str(min_time), str(max_time))\n",
    "    event_type = \"CE\"\n",
    "    events = Fido.search(\n",
    "        time_requested_for_batch, a.hek.EventType(event_type)\n",
    "    )\n",
    "    return events\n",
    "\n",
    "def get_images(min_time, max_time) -> dict:#\n",
    "    time_requested_for_batch = a.Time(str(min_time), str(max_time))\n",
    "    images_cor1 = Fido.search(\n",
    "        time_requested_for_batch, a.Instrument(\"SECCHI\"), a.Detector(\"COR1\")\n",
    "    )\n",
    "    images_cor2 = Fido.search(\n",
    "        time_requested_for_batch, a.Instrument(\"SECCHI\"), a.Detector(\"COR2\")\n",
    "    )\n",
    "    images_lasco_c2 = Fido.search(\n",
    "        time_requested_for_batch, a.Instrument(\"LASCO\"), a.Detector(\"C2\")\n",
    "    )\n",
    "    images_lasco_c3 = Fido.search(\n",
    "        time_requested_for_batch, a.Instrument(\"LASCO\"), a.Detector(\"C3\")\n",
    "    )\n",
    "    \n",
    "    return {\"cor1\": images_cor1, \"cor2\": images_cor2, \"lasco_c2\": images_lasco_c2, \"lasco_c3\": images_lasco_c3}\n",
    "\n",
    "def generate_preview_and_video(folder, pattern=\"*.fts\", out=\"event.avi\"):\n",
    "    ims = natsorted(glob(os.path.join(folder, pattern)))\n",
    "\n",
    "    if len(ims) == 0:\n",
    "        logger.info(\"No images found matching pattern\")\n",
    "        return\n",
    "        \n",
    "    vid_path = os.path.join(folder, out)\n",
    "    writer = cv2.VideoWriter(vid_path, apiPreference=cv2.CAP_FFMPEG, frameSize=fitsio.getdata(ims[0]).shape, fourcc=cv2.VideoWriter_fourcc(*\"MJPG\"), fps=3)\n",
    "    \n",
    "    for i, im in enumerate(ims):\n",
    "\n",
    "        try:\n",
    "            header = fitsio.getheader(im)\n",
    "            \n",
    "            raw_image  = np.log(fitsio.getdata(im).astype(np.float32))\n",
    "            scaled = (raw_image - raw_image.min()) / (raw_image.max() - raw_image.min() + 1e-15)\n",
    "        \n",
    "            scaled *= 255\n",
    "            scaled = scaled.astype(np.uint8)\n",
    "        \n",
    "            base = os.path.splitext(os.path.basename(im))[0]\n",
    "            colored = cv2.applyColorMap(scaled, cv2.COLORMAP_INFERNO)\n",
    "            cv2.imwrite(os.path.join(folder, base+\".jpg\"), colored)\n",
    "            writer.write(colored)\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    writer.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a367ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "ephem_dir = os.path.dirname(\"./\")\n",
    "l5_positions_fname = os.path.join(ephem_dir, \"L5_positions.ephemeris\")\n",
    "stereoA_positions_fname = os.path.join(ephem_dir, \"StereoA_positions.ephemeris\")\n",
    "stereoB_positions_fname = os.path.join(ephem_dir, \"StereoB_positions.ephemeris\")\n",
    "SOHO_positions_fname = os.path.join(ephem_dir, \"SOHO_positions.ephemeris\")\n",
    "\n",
    "# Get data\n",
    "l5_positions_data = load_ephemeris_data(l5_positions_fname)\n",
    "stereoA_positions_data = load_ephemeris_data(stereoA_positions_fname)\n",
    "stereoB_positions_data = load_ephemeris_data(stereoB_positions_fname)\n",
    "SOHO_positions_data = load_ephemeris_data(SOHO_positions_fname)\n",
    "# TODO: Create a method to find the dataframe without requiring equal time\n",
    "l5_time, L5x, L5y, L5z, L5r = data_to_vectors(l5_positions_data)\n",
    "_, SAx, SAy, SAz, SAr = data_to_vectors(stereoA_positions_data, l5_time)\n",
    "_, SBx, SBy, SBz, SBr = data_to_vectors(stereoB_positions_data, l5_time)\n",
    "_, Sohox, Sohoy, Sohoz, Sohor = data_to_vectors(SOHO_positions_data, l5_time)\n",
    "\n",
    "initial_colnames = [\n",
    "    \"L5 x [km]\",\n",
    "    \"L5 y [km]\",\n",
    "    \"L5 z [km]\",\n",
    "    \"L5 r [km]\",\n",
    "    \"SA x [km]\",\n",
    "    \"SA y [km]\",\n",
    "    \"SA z [km]\",\n",
    "    \"SA r [km]\",\n",
    "    \"SB x [km]\",\n",
    "    \"SB y [km]\",\n",
    "    \"SB z [km]\",\n",
    "    \"SB r [km]\",\n",
    "    \"SOHO x [km]\",\n",
    "    \"SOHO y [km]\",\n",
    "    \"SOHO z [km]\",\n",
    "    \"SOHO r [km]\",\n",
    "]\n",
    "initial_data = (\n",
    "    np.asarray(\n",
    "        [\n",
    "            L5x,\n",
    "            L5y,\n",
    "            L5z,\n",
    "            L5r,\n",
    "            SAx,\n",
    "            SAy,\n",
    "            SAz,\n",
    "            SAr,\n",
    "            SBx,\n",
    "            SBy,\n",
    "            SBz,\n",
    "            SBr,\n",
    "            Sohox,\n",
    "            Sohoy,\n",
    "            Sohoz,\n",
    "            Sohor,\n",
    "        ]\n",
    "    )\n",
    ").T\n",
    "\n",
    "df = pd.DataFrame(initial_data, index=l5_time, columns=initial_colnames)\n",
    "df[\"Distance L5 Stereo A [km]\"] = np.sqrt(\n",
    "    (df[\"L5 x [km]\"] - df[\"SA x [km]\"]) ** 2\n",
    "    + (df[\"L5 y [km]\"] - df[\"SA y [km]\"]) ** 2\n",
    "    + (df[\"L5 z [km]\"] - df[\"SA z [km]\"]) ** 2\n",
    ")\n",
    "df[\"Distance L5 Stereo B [km]\"] = np.sqrt(\n",
    "    (df[\"L5 x [km]\"] - df[\"SB x [km]\"]) ** 2\n",
    "    + (df[\"L5 y [km]\"] - df[\"SB y [km]\"]) ** 2\n",
    "    + (df[\"L5 z [km]\"] - df[\"SB z [km]\"]) ** 2\n",
    ")\n",
    "df[\"Stereo AB Angle [deg]\"] = (\n",
    "    np.arccos(\n",
    "        (\n",
    "            df[\"SA x [km]\"] * df[\"SB x [km]\"]\n",
    "            + df[\"SA y [km]\"] * df[\"SB y [km]\"]\n",
    "            + df[\"SA z [km]\"] * df[\"SB z [km]\"]\n",
    "        )\n",
    "        / (df[\"SA r [km]\"] * df[\"SB r [km]\"])\n",
    "    )\n",
    "    * 180\n",
    "    / np.pi\n",
    ")\n",
    "df[\"Stereo A Soho Angle [deg]\"] = (\n",
    "    np.arccos(\n",
    "        (\n",
    "            df[\"SA x [km]\"] * df[\"SOHO x [km]\"]\n",
    "            + df[\"SA y [km]\"] * df[\"SOHO y [km]\"]\n",
    "            + df[\"SA z [km]\"] * df[\"SOHO z [km]\"]\n",
    "        )\n",
    "        / (df[\"SA r [km]\"] * df[\"SOHO r [km]\"])\n",
    "    )\n",
    "    * 180\n",
    "    / np.pi\n",
    ")\n",
    "df[\"Stereo B Soho Angle [deg]\"] = (\n",
    "    np.arccos(\n",
    "        (\n",
    "            df[\"SB x [km]\"] * df[\"SOHO x [km]\"]\n",
    "            + df[\"SB y [km]\"] * df[\"SOHO y [km]\"]\n",
    "            + df[\"SB z [km]\"] * df[\"SOHO z [km]\"]\n",
    "        )\n",
    "        / (df[\"SB r [km]\"] * df[\"SOHO r [km]\"])\n",
    "    )\n",
    "    * 180\n",
    "    / np.pi\n",
    ")\n",
    "\n",
    "# Parameters to define L5 geometry\n",
    "earth_l5_angle_degrees = 60\n",
    "error_range_degrees = 10\n",
    "required_distance_to_l5_km = 50000000  # km\n",
    "max_angle_between_crafts_deg = earth_l5_angle_degrees + error_range_degrees\n",
    "min_angle_between_crafts_deg = earth_l5_angle_degrees - error_range_degrees\n",
    "\n",
    "min_angle_between_crafts_deg\n",
    "\n",
    "approx_date_last_B_contact = date(2016, 9, 1)  # some time September 2016\n",
    "\n",
    "# All angles where Stereo AB are 60 degrees apart\n",
    "df_angles_fit = df.query(\n",
    "    \"`Stereo AB Angle [deg]` >= {} & `Stereo AB Angle [deg]` <= {}\".format(\n",
    "        min_angle_between_crafts_deg, max_angle_between_crafts_deg\n",
    "    )\n",
    ")\n",
    "\n",
    "# Perfect fits if it ever exists\n",
    "#df_angles_perfect_fit = df.query(\n",
    "#    \"abs(`Stereo AB Angle [deg]` - {}) <= {}\".format(earth_l5_angle_degrees, 1e-6)\n",
    "#)\n",
    "\n",
    "df_stereo_soho_angles_fit = df.query(\n",
    "    \"`Stereo B Soho Angle [deg]` <= {} & `Stereo A Soho Angle [deg]` <= {}\".format(\n",
    "        max_angle_between_crafts_deg, max_angle_between_crafts_deg\n",
    "    )\n",
    ")\n",
    "\n",
    "df_StereoB_close = df.query(\n",
    "    \"`Distance L5 Stereo B [km]` <= {}\".format(required_distance_to_l5_km)\n",
    ")\n",
    "df_StereoA_close = df.query(\n",
    "    \"`Distance L5 Stereo A [km]` <= {}\".format(required_distance_to_l5_km)\n",
    ")\n",
    "\n",
    "# get times from df_{}.index\n",
    "# Main point: Do angles fit? - more important than distance for now.\n",
    "angle_AB_index = df_angles_fit.index\n",
    "angle_ABSOHO_index = df_stereo_soho_angles_fit.index\n",
    "\n",
    "stereoab = set([d for d in df_angles_fit.index])\n",
    "soho_stereoab = set([d for d in df_stereo_soho_angles_fit.index])\n",
    "\n",
    "overlap_dates = sorted(list(stereoab.intersection(soho_stereoab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15a01c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = \"/home/josh/code/2023-europe-space-weather/config\"\n",
    "\n",
    "with open(os.path.join(config_path, \"onboard.yaml\"), \"r\") as f:\n",
    "    data_path = yaml.load(f, Loader=yaml.Loader)[\"drive_locations\"][\"datapath\"]\n",
    "\n",
    "cor1_folder = os.path.join(data_path, \"data\", \"cor1\")\n",
    "cor2_folder = os.path.join(data_path, \"data\", \"cor2\")\n",
    "events_folder = os.path.join(data_path, \"data\", \"events\")\n",
    " \n",
    "try:\n",
    "    os.makedirs(events_folder, exist_ok=True)\n",
    "except OSError as e:\n",
    "    logging.error(\"Error when creating Folder = {} - {}\".format(event_folder, e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684bebd2-0c6e-45a0-a38a-1ac55935942f",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_type = lambda x: os.path.splitext(os.path.basename(x))[0].split('_')[-1][0]\n",
    "stereo_satellite = lambda x: os.path.splitext(os.path.basename(x))[0].split('_')[-1][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52ce0b6-5852-4da7-9a09-b6756e7052cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_previews(root):\n",
    "    for satellite in os.listdir(root):\n",
    "        if os.path.isfile(satellite):\n",
    "            continue\n",
    "    \n",
    "        for angle in os.listdir(os.path.join(root, satellite)):\n",
    "            generate_preview_and_video(os.path.join(root, satellite, angle))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37cf9553-7cb0-49df-849f-9cb277a1429e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_folder_pol(folder):\n",
    "    for image in glob(os.path.join(folder, \"*.fts\")):\n",
    "        imtype = image_type(image)\n",
    "        sat = stereo_satellite(image).lower()\n",
    "\n",
    "        polar_angle = fitsio.getheader(image)['POLAR']\n",
    "        pol_folder = os.path.join(folder, \"stereo_\"+str(sat), str(polar_angle))\n",
    "        os.makedirs(pol_folder, exist_ok=True)\n",
    "        shutil.copy2(image, pol_folder)\n",
    "        os.remove(image)\n",
    "\n",
    "        if len(os.listdir(folder)) == 0:\n",
    "            shutil.rmtree(folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f4ab9d-b5ec-4d95-b70b-79bc6830932e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_folder_to_gcs(source_folder_path, bucket, destination_folder_path=''):\n",
    "\n",
    "    # Ensure the destination folder path ends with \"/\"\n",
    "    if destination_folder_path and not destination_folder_path.endswith(\"/\"):\n",
    "        destination_folder_path += \"/\"\n",
    "\n",
    "    for root, _, files in os.walk(source_folder_path):\n",
    "        for file in files:\n",
    "            # Calculate source file path\n",
    "            source_file_path = os.path.join(root, file)\n",
    "\n",
    "            # Calculate destination file path in GCS\n",
    "            destination_blob_name = os.path.join(destination_folder_path, os.path.relpath(source_file_path, source_folder_path))\n",
    "\n",
    "            # Create a blob from the source file and copy it to the destination\n",
    "            blob = bucket.blob(destination_blob_name)\n",
    "            blob.upload_from_filename(source_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3d379e-4761-4655-aa56-00753e0ee0fe",
   "metadata": {},
   "source": [
    "!gcloud auth application-default login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674d0957-1b41-4681-a3d6-40e9f0d42f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud auth application-default set-quota-project fdl-europe-space-weather\n",
    "os.environ['GOOGLE_CLOUD_PROJECT'] = 'fdl-europe-space-weather'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b75d6de-63b8-490b-8d38-5c215d7f589c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage    \n",
    "client = storage.Client(project='fdl-europe-space-weather')\n",
    "bucket = client.get_bucket('fdl_space_weather_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c31e35-7217-4c97-ba96-5abb4f85aeea",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_folder = \"/media/josh/josh_tuf_a/data/fdl/2023/onboard/\"\n",
    "\n",
    "try:\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "except OSError as e:\n",
    "    logging.error(\"Error when creating Folder = {} - {}\".format(event_folder, e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1c2cbb-1c05-4a6c-9a0a-5d4badc46b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "_start = datetime.strptime(\"20140301_00:00:00\", \"%Y%m%d_%H:%M:%S\")\n",
    "start = _start\n",
    "end = start + timedelta(days=1)\n",
    "\n",
    "while end < _start + timedelta(days=30):\n",
    "\n",
    "    logger.info(f\"Searching range {start} to {end}\")\n",
    "    \n",
    "    res = get_images(start, end)\n",
    "    capture_date = start.strftime(\"%Y%m%d\")\n",
    "    event_path = os.path.join(output_folder, capture_date)\n",
    "    \n",
    "    if len(res['cor1']) != 0:\n",
    "        folder = os.path.join(event_path, \"cor1\")\n",
    "        logger.info(f\"Starting Cor1 Download to {folder}\")\n",
    "        download_batch(res[\"cor1\"], folder)\n",
    "        sort_folder_pol(folder)\n",
    "        #make_previews(folder)\n",
    "    \n",
    "    if len(res['cor2']) != 0:\n",
    "        folder = os.path.join(event_path, \"cor2\")\n",
    "        logger.info(f\"Starting Cor2 Download to {folder}\")\n",
    "        download_batch(res[\"cor2\"], folder)\n",
    "        sort_folder_pol(folder)\n",
    "        #make_previews(folder)\n",
    "    \n",
    "    logger.info(\"Starting Lasco C2 Download\")\n",
    "    folder = os.path.join(event_path, \"lasco_c2\")\n",
    "    download_batch(res[\"lasco_c2\"], folder)\n",
    "    #generate_preview_and_video(folder, \"*.fts\", out=\"event.avi\")\n",
    "    \n",
    "    logger.info(\"Starting Lasco C3 Download\")\n",
    "    folder = os.path.join(event_path, \"lasco_c3\")\n",
    "    download_batch(res[\"lasco_c3\"], folder)\n",
    "    #generate_preview_and_video(folder, \"*.fts\", out=\"event.avi\")\n",
    "    \n",
    "    logger.info(\"Pushing to bucket\")\n",
    "    #copy_folder_to_gcs(event_path, bucket, f'space/{capture_date}')\n",
    "    #shutil.rmtree(event_path)\n",
    "    \n",
    "    start += timedelta(days=1)\n",
    "    end += timedelta(days=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e677a2-5f00-49aa-afeb-51e36aa5736e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!gustil -m rsync -r <output_folder> gs://fdl_space_weather_data/onboard"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
