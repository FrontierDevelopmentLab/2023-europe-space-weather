{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0456c74",
   "metadata": {},
   "source": [
    "# Scrape online catalogues to get expert CME labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7930b0c8",
   "metadata": {},
   "source": [
    "* Currently based on cor1 https://cor1.gsfc.nasa.gov/catalog/\n",
    "* (To be extended to others listed in http://solar.jhuapl.edu/Data-Products/COR-CME-Catalog.php)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b12f4d",
   "metadata": {},
   "source": [
    "## Cor1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44167368-f923-4d0b-a4a5-07951754683f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "import requests\n",
    "from datetime import datetime\n",
    "from tqdm.auto import tqdm\n",
    "from urllib.parse import urljoin\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import json\n",
    "\n",
    "# <3 ChatGPT\n",
    "def extract_cardinal_direction(text):\n",
    "    direction_pattern = re.compile(r'\\b(N|NNE|NE|ENE|E|ESE|SE|SSE|S|SSW|SW|WSW|W|WNW|NW|NNW)\\b', re.IGNORECASE)\n",
    "    match = direction_pattern.search(text)\n",
    "    \n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "def extract_timestamp_and_urls(text):\n",
    "    url_pattern = re.compile(r'urls\\[\\d+\\]=(.*?);', re.DOTALL)\n",
    "    urls = url_pattern.findall(text)\n",
    "\n",
    "    timestamps = []\n",
    "    \n",
    "    for url in urls:\n",
    "        url = url.strip(' \"\\'')\n",
    "        timestamp_str = re.search(r'(\\d{8}_\\d{6})\\.png', url).group(1)\n",
    "        timestamps.append(timestamp_str)\n",
    "\n",
    "    return timestamps[0], timestamps[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c3b6cc-c561-4a70-b780-b4b903b1177d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_event(cols):\n",
    "    \n",
    "    link = cols[1].find('a')\n",
    "    \n",
    "    event = {}\n",
    "    \n",
    "    event['datetime'] = f\"{cols[0].text} {cols[2].text}\"\n",
    "    event['type'] = link.text\n",
    "\n",
    "    comment = cols[3].text\n",
    "\n",
    "    event['faint'] = 'faint' in comment\n",
    "    event['narrow'] = 'narrow' in comment\n",
    "    event['wide'] = 'wide' in comment\n",
    "    event['fast'] = 'fast' in comment\n",
    "    event['visible'] = not 'seen' in comment\n",
    "    event['direction'] = extract_cardinal_direction(comment)\n",
    "\n",
    "    video = urljoin(url, link[\"href\"]).strip()            \n",
    "    tstart, tend = extract_timestamp_and_urls(requests.get(video).text)\n",
    "    event['event_start_time'] = tstart\n",
    "    event['event_stop_time'] = tend\n",
    "\n",
    "    return event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3726f202-608f-4e3e-ad6d-621fba6bcc79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_event_row(catalog_page) -> dict:\n",
    "    \n",
    "    soup = bs4.BeautifulSoup(requests.get(catalog_page).text)\n",
    "    rows = soup.find_all('tr')\n",
    "    \n",
    "    out = defaultdict(list)\n",
    "    \n",
    "    pbar = tqdm(rows)\n",
    "    \n",
    "    for row in pbar:\n",
    "        cols = row.find_all('td')\n",
    "        \n",
    "        if len(cols) < 2:\n",
    "            continue\n",
    "            \n",
    "        link = cols[1].find('a')\n",
    "        \n",
    "        if link is None:\n",
    "            continue\n",
    "        \n",
    "        if 'CME' in link.text:\n",
    "            # Stereo A\n",
    "            a = extract_event(cols[:6])\n",
    "            \n",
    "            # Stereo B\n",
    "            b = extract_event(cols[6:])\n",
    "            \n",
    "            out['stereo_a'].append(a)\n",
    "            out['stereo_b'].append(b)\n",
    "                \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494e81d9-64ea-452a-a3f3-7a66c63433de",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://cor1.gsfc.nasa.gov/catalog/cme/2014/Daniel_Hong_COR1_preliminary_event_list_2014-02.html\"\n",
    "events = get_event_row(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5894e2b-99fe-4843-9470-eebb2df164e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79daf2d6-f13e-43bc-b42c-f7ad368e5498",
   "metadata": {},
   "outputs": [],
   "source": [
    "date_folders = glob(\"/media/josh/josh_tuf_a/data/fdl/2023/onboard/*\")\n",
    "os.makedirs(\"/media/josh/josh_tuf_a/data/fdl/2023/wrong\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25fb6086-64f9-4f78-8f60-fcc3dddcbb17",
   "metadata": {},
   "outputs": [],
   "source": [
    "for date_folder in date_folders:\n",
    "\n",
    "    date = os.path.basename(date_folder)\n",
    "\n",
    "    for objects in os.walk(top = date_folder):\n",
    "        dirpath, dirnames, filenames = objects\n",
    "        if 'cor' not in dirpath:\n",
    "            continue\n",
    "        \n",
    "        for file in filenames:\n",
    "            if date not in file and date[:4] == file[:4]:\n",
    "                shutil.move(os.path.join(dirpath, file), \"/media/josh/josh_tuf_a/data/fdl/2023/wrong\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee53b13-0414-42c7-b752-408e4304d47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.path.basename(date_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18d4ba7-a80a-4f27-901b-4384f2f8fc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "date"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eab963f",
   "metadata": {},
   "source": [
    "## Cor2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f66b501",
   "metadata": {},
   "source": [
    "# http://spaceweather.gmu.edu/seeds/secchi.php"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18bd53f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://spaceweather.gmu.edu/seeds/monthly.php?a=2014&b=02&cor2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a79132",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_timestamps(event_data):\n",
    "    # Split the event data into lines\n",
    "    event_lines = event_data.strip().split(\"\\n\")\n",
    "\n",
    "    # Extract timestamps from the uncommented events\n",
    "    event_timestamps = [line.split()[0] + \" \" + line.split()[1] for line in event_lines if not line.startswith(\"#\")]\n",
    "\n",
    "    # Convert timestamps to datetime objects\n",
    "    event_datetimes = [datetime.strptime(timestamp, \"%Y/%m/%d %H:%M:%S\") for timestamp in event_timestamps]\n",
    "\n",
    "    # return in string format as above for cor1\n",
    "    date_format = \"%Y%m%d_%H%M%S\"\n",
    "\n",
    "    return event_datetimes[0].strftime(date_format), event_datetimes[-1].strftime(date_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9218ecdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# output format same as above cor1\n",
    "out = defaultdict(list)\n",
    "\n",
    "# Send an HTTP GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Parse the HTML content using Beautiful Soup\n",
    "soup = bs4.BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "# Find all <a> tags with \"href\" containing \"detection_cor2\"\n",
    "date_links = soup.find_all(\"a\", href=lambda href: href and \"detection_cor2\" in href)\n",
    "\n",
    "# Extract and print the dates from the links\n",
    "for link in date_links:\n",
    "    date = link.text.strip()\n",
    "    # only retrieve the date links\n",
    "    if \"/\" in date:\n",
    "        #print(date, link)\n",
    "        full_url = urljoin(url, link[\"href\"]).strip()\n",
    "        event_data = requests.get(full_url).text\n",
    "        #print(full_url)\n",
    "        #print(event_data)\n",
    "        tstart, tend = extract_timestamps(event_data)\n",
    "        print(tstart, tend)\n",
    "\n",
    "        o = {\n",
    "            'event_start_time': tstart,\n",
    "            'event_stop_time': tend,\n",
    "             }\n",
    "\n",
    "        if \"/a/\" in full_url:\n",
    "            out['stereo_a'].append(o)\n",
    "        elif \"/b/\" in full_url:\n",
    "            out['stereo_b'].append(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33eec577",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./events_201402cor2.json', 'w') as fp:\n",
    "    json.dump(out, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b4e713",
   "metadata": {},
   "outputs": [],
   "source": [
    "out.keys()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
